{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>   PDLLMs: A group of tailored DNA large language models for analyzing plant genomes   </p>"},{"location":"#demo-for-plant-dna-llms-prediction","title":"Demo for plant DNA LLMs prediction","text":"<p>Online prediction of other models and prediction tasks can be found here.</p>"},{"location":"citation/","title":"Citation","text":"<p>If you use our models/datasets, codes for finetune/inference or the web applications for prediction, please cite our article.</p> <ul> <li>Liu GQ, Chen L, Wu YC, Han YS, Bao Y, Zhang T*. PDLLMs: A group of tailored DNA large language models for analyzing plant genomes. Molecular Plant DOI: https://doi.org/10.1016/j.molp.2024.12.0066</li> </ul>"},{"location":"docker_implementation_for_model_inference/","title":"Docker implementation for model inference","text":"<p>Environment deployment for LLMs may be an arduous job. To simplify this process, we also provide a docker version of our model inference code.</p> <p>The images of the docker version are here, and the usage of docker implementation is shown below.</p>"},{"location":"docker_implementation_for_model_inference/#inference-using-gpu","title":"Inference using GPU","text":"<p>For GPU inference (with Nvidia GPU), please pull the image with <code>gpu</code> tag, and make sure your computer has install the Nvidia Container Toolkit.</p> <p>First download a finetune model from Huggingface or ModelScope, here we use Plant DNAMamba model as an example to predict active core promoters\u3002</p> <pre><code># prepare a work directory\nmkdir LLM_inference\ncd LLM_inference\ngit clone https://huggingface.co/zhangtaolab/plant-dnamamba-BPE-promoter\n</code></pre> <p>Then download the corresponding dataset, and if users have their own data, users can also prepare a custom dataset based on the previously mentioned inference data format.</p> <pre><code>git clone https://huggingface.co/datasets/zhangtaolab/plant-multi-species-core-promoters\n</code></pre> <p>Once the model and dataset are ready, pull our model inference image from docker and test if it works.</p> <pre><code>docker pull zhangtaolab/plant_llms_inference:gpu\ndocker run --runtime=nvidia --gpus=all -v ./:/home/llms zhangtaolab/plant_llms_inference:gpu -h\n</code></pre> <pre><code>usage: inference.py [-h] [-v] -m MODEL [-f FILE] [-s SEQUENCE] [-t THRESHOLD]\n                    [-l MAX_LENGTH] [-bs BATCH_SIZE] [-p SAMPLE] [-seed SEED]\n                    [-d {cpu,gpu,mps,auto}] [-o OUTFILE] [-n]\n\nScript for Plant DNA Large Language Models (LLMs) inference\n\noptions:\n  -h, --help            show this help message and exit\n  -v, --version         show program's version number and exit\n  -m MODEL              Model path (should contain both model and tokenizer)\n  -f FILE               File contains sequences that need to be classified\n  -s SEQUENCE           One sequence that need to be classified\n  -t THRESHOLD          Threshold for defining as True class (Default: 0.5)\n  -l MAX_LENGTH         Max length of tokenized sequence (Default: 512)\n  -bs BATCH_SIZE        Batch size for classification (Default: 1)\n  -p SAMPLE             Subsampling for testing (Default: 1e7)\n  -seed SEED            Random seed for subsampling (Default: None)\n  -d {cpu,gpu,mps,auto}\n                        Choose CPU or GPU to do inference (require specific\n                        drivers) (Default: auto)\n  -o OUTFILE            Prediction results (Default: stdout)\n  -n                    Whether or not save the runtime locally (Default:\n                        False)\n\nExample:\n  docker run --runtime=nvidia --gpus=all -v /local:/container zhangtaolab/plant_llms_inference:gpu -m model_path -f seqfile.csv -o output.txt\n  docker run --runtime=nvidia --gpus=all -v /local:/container zhangtaolab/plant_llms_inference:gpu -m model_path -s 'ATCGGATCTCGACAGT' -o output.txt\n</code></pre> <p>If the preceding information is displayed, the image is downloaded and the inference script can run normally.  Inference is performed below using previously prepared models and datasets.</p> <pre><code>docker run --runtime=nvidia --gpus=all -v ./:/home/llms zhangtaolab/plant_llms_inference:gpu -m /home/llms/plant-dnamamba-BPE-promoter -f /home/llms/plant-multi-species-core-promoters/test.csv -o /home/llms/predict_results.txt\n</code></pre> <p>After the inference progress bar is completed, see the output file <code>predict_results.txt</code> in the current local directory, which saves the prediction results corresponding to each sequence in the input file.</p>"},{"location":"docker_implementation_for_model_inference/#inference-using-cpu","title":"Inference using CPU","text":"<p>For CPU inference,  please pull the image with <code>cpu</code> tag, this image support computer without NVIDIA GPU, such as cpu-only or Apple M-series Silicon. (Note that Inference of DNAMamba model is not supported in CPU mode)</p> <p>First download a finetune model from Huggingface or ModelScope, here we use Plant DNAGPT model as an example to predict active core promoters\u3002</p> <pre><code># prepare a work directory\nmkdir LLM_inference\ncd LLM_inference\ngit clone https://huggingface.co/zhangtaolab/plant-dnagpt-BPE-promoter\n</code></pre> <p>Then download the corresponding dataset, and if users have their own data, users can also prepare a custom dataset based on the previously mentioned inference data format.</p> <pre><code>git clone https://huggingface.co/datasets/zhangtaolab/plant-multi-species-core-promoters\n</code></pre> <p>Once the model and dataset are ready, pull our model inference image from docker and test if it works.</p> <pre><code>docker pull zhangtaolab/plant_llms_inference:cpu\ndocker run -v ./:/home/llms zhangtaolab/plant_llms_inference:cpu -h\n</code></pre> <pre><code>usage: inference.py [-h] [-v] -m MODEL [-f FILE] [-s SEQUENCE] [-t THRESHOLD]\n                    [-l MAX_LENGTH] [-bs BATCH_SIZE] [-p SAMPLE] [-seed SEED]\n                    [-d {cpu,gpu,mps,auto}] [-o OUTFILE] [-n]\n\nScript for Plant DNA Large Language Models (LLMs) inference\n\noptions:\n  -h, --help            show this help message and exit\n  -v, --version         show program's version number and exit\n  -m MODEL              Model path (should contain both model and tokenizer)\n  -f FILE               File contains sequences that need to be classified\n  -s SEQUENCE           One sequence that need to be classified\n  -t THRESHOLD          Threshold for defining as True class (Default: 0.5)\n  -l MAX_LENGTH         Max length of tokenized sequence (Default: 512)\n  -bs BATCH_SIZE        Batch size for classification (Default: 1)\n  -p SAMPLE             Subsampling for testing (Default: 1e7)\n  -seed SEED            Random seed for subsampling (Default: None)\n  -d {cpu,gpu,mps,auto}\n                        Choose CPU or GPU to do inference (require specific\n                        drivers) (Default: auto)\n  -o OUTFILE            Prediction results (Default: stdout)\n  -n                    Whether or not save the runtime locally (Default:\n                        False)\n\nExample:\n  docker run -v /local:/container zhangtaolab/plant_llms_inference:gpu -m model_path -f seqfile.csv -o output.txt\n  docker run -v /local:/container zhangtaolab/plant_llms_inference:gpu -m model_path -s 'ATCGGATCTCGACAGT' -o output.txt\n</code></pre> <p>If the preceding information is displayed, the image is downloaded and the inference script can run normally.  Inference is performed below using previously prepared models and datasets.</p> <pre><code>docker run -v ./:/home/llms zhangtaolab/plant_llms_inference:cpu -m /home/llms/plant-dnagpt-BPE-promoter -f /home/llms/plant-multi-species-core-promoters/test.csv -o /home/llms/predict_results.txt\n</code></pre> <p>After the inference progress bar is completed, see the output file <code>predict_results.txt</code> in the current local directory, which saves the prediction results corresponding to each sequence in the input file.</p> <ul> <li>The detailed usage is the same as the section Inference.</li> </ul>"},{"location":"docker_implementation_for_model_inference/#inference-with-gui","title":"Inference with GUI","text":"<p>For convience, we also allow users predicting locally with a GUI based on Gradio, a friendly web app for machine learning models.</p> <p>CPU inference can simply run the following command, then open the url <code>http://127.0.0.1:7860</code> in your browser, then you will see a GUI with several options for task prediction. (plant DNAMamba models are not shown in the cpu image because CPU cannot infer these models)</p> <pre><code>mkdir -p llms_gradio/cache\ncd llms_gradio\ndocker run -p 7860:7860 -v ./cache:/root/.cache --name gradio_cpu zhangtaolab/plant_llms_gradio:cpu\n</code></pre> <p>Models will be downloaded into the <code>llms_gradio/cache</code> folder in your computer during inference.</p> <p>GPU-based inference requires users to install the Nvidia Container Toolkit in advance.</p> <p>After the environment is prepared completely, run the following command, then open the url <code>http://127.0.0.1:7860</code> in your browser.</p> <pre><code>mkdir -p llms_gradio/cache\ncd llms_gradio\ndocker run --gpus=all -p 7860:7860 -v ./cache:/root/.cache --name gradio_gpu zhangtaolab/plant_llms_gradio:gpu\n</code></pre>"},{"location":"docker_implementation_for_model_inference/#online-prediction-platform","title":"Online prediction platform","text":"<p>In order to facilitate users to use the model to predict DNA analysis tasks, we also provide online prediction platforms.</p> <p>Please refer to online prediction platform</p>"},{"location":"environment/","title":"Environment","text":"<p>Anaconda package manager is recommended for building the training environment. For pre-train and fine-tune models, please ensure that you have a Nvidia GPU and the corresponding drivers are installed. For inference, devices without Nvidia GPU (CPU only, AMD GPU, Apple Silion, etc.) are also acceptable.</p>"},{"location":"environment/#11-download-and-install-anaconda-package-manager","title":"1.1 Download and install Anaconda package manager","text":""},{"location":"environment/#12-create-environment-we-trained-the-models-with-python-311","title":"1.2 Create environment (We trained the models with python 3.11)","text":"<pre><code>conda create -n llms python=3.11\nconda activate llms\n</code></pre>"},{"location":"environment/#13-install-dependencies","title":"1.3 Install dependencies","text":"<p>If you want to pre-train or fine-tune models, make sure you are using Nvidia GPU(s). Install Nvidia driver and corresponding version of CUDA driver (&gt; 11.0, we used CUDA 12.1).  </p> <p>Also Pytorch (&gt;=2.0) with corresponding CUDA version should also install. We recommend to use <code>pip</code> to install python packages that needed. Please be sure to install the corresponding CUDA and Torch versions carefully, the CUDA version used in this test environment is 12.1. Please refer to Official Website for the detailed installation tutorial of pytorch.</p> <pre><code>pip install 'torch&lt;2.5'\n</code></pre> <p>If you just want to use models for inference (prediction), you can install Pytorch GPU version (above) or install Pytorch CPU version if your machine has no Nvidia GPU.</p> <pre><code>pip install 'torch&lt;2.5'\n</code></pre> <p>Next install other required dependencies.</p> <pre><code>git clone --recursive https://github.com/zhangtaolab/Plant_DNA_LLMs\ncd Plant_DNA_LLMs\npip install -r requirements.txt\n</code></pre> <p>(Optional) If you want to train a mamba model, you need to install several extra dependencies, also you should have a Nvidia GPU.</p> <pre><code>pip install 'causal-conv1d&lt;=1.3'\npip install 'mamba-ssm&lt;2'\n</code></pre>"},{"location":"environment/#14-install-git-lfs","title":"1.4 Install git-lfs","text":"<p><code>glt-lfs</code> is required for download large models and datasets\uff0c<code>git-lfs</code> installation can be refer to git-lfs install.</p> <p>If <code>git-lfs</code> is installed, run the following command</p> <pre><code>$ git lfs version\n</code></pre> <p>will get message like this</p> <pre><code>git-lfs/3.3.0 (GitHub; linux amd64; go 1.19.8)\n</code></pre>"},{"location":"fine_tune/","title":"Fine tune","text":"<p>To fine-tune the plant DNA LLMs, please first download the desired models from HuggingFace or ModelScope to local. You can use <code>git clone</code> (which may require <code>git-lfs</code> to be installed) to retrieve the model or directly download the model from the website.</p> <p>In the activated <code>llms</code> python environment, use the <code>model_finetune.py</code> script to fine-tune a model for downstream task.  </p> <p>Our script accepts <code>.csv</code> format data (separated by <code>,</code>) as input, when preparing the training data, please make sure the data contain a header and at least these two columns:</p> <pre><code>sequence,label\n</code></pre> <p>Where <code>sequence</code> is the input sequence, and <code>label</code> is the corresponding label for the sequence.</p> <p>We also provide several plant genomic datasets for fine-tuning on the HuggingFace and ModelScope.</p> <ul> <li>Here is the pretrain models list</li> </ul> <p>We use Plant DNAGPT model as example to fine-tune a model for active core promoter prediction.</p> <p>First download a pretrain model and corresponding dataset from HuggingFace or ModelScope:</p> <pre><code># prepare a output directory\nmkdir finetune\n# download pretrain model\ngit clone https://huggingface.co/zhangtaolab/plant-dnagpt-BPE models/plant-dnagpt-BPE\n# download train dataset\ngit clone https://huggingface.co/datasets/zhangtaolab/plant-multi-species-core-promoters data/plant-multi-species-core-promoters\n</code></pre> <ul> <li>Note: If downloading from huggingface encounters network error, please try to download model/dataset from ModelScope or change to the accelerate mirror before downloading.</li> </ul> <pre><code># Download with git\ngit clone https://hf-mirror.com/[organization_name/repo_name]\n# Download with huggingface-cli\nexport HF_ENDPOINT=\"https://hf-mirror.com\"\nhuggingface-cli download [organization_name/repo_name]\n</code></pre> <p>After preparing the model and dataset, using the following script to finetune model (here is a promoter prediction example)</p> <pre><code>python model_finetune.py \\\n    --model_name_or_path plant-dnagpt-BPE \\\n    --train_data plant-multi-species-core-promoters/train.csv \\\n    --test_data plant-multi-species-core-promoters/test.csv \\\n    --eval_data plant-multi-species-core-promoters/dev.csv \\\n    --train_task classification \\\n    --labels 'Not promoter;Core promoter' \\\n    --run_name plant_dnagpt_BPE_promoter \\\n    --per_device_train_batch_size 4 \\\n    --per_device_eval_batch_size 8 \\\n    --learning_rate 1e-5 \\\n    --num_train_epochs 5 \\\n    --load_best_model_at_end \\\n    --metric_for_best_model 'f1' \\\n    --save_strategy epoch \\\n    --logging_strategy epoch \\\n    --evaluation_strategy epoch \\\n    --output_dir plant-dnagpt-BPE-promoter\n</code></pre> <p>In this script: 1. <code>--model_name_or_path</code>: Path to the foundation model you downloaded 2. <code>--train_data</code>: Path to the train dataset 3. <code>--test_data</code>: Path to the test dataset, omit it if no test data available 4. <code>--dev_data</code>: Path to the validation dataset, omit it if no validation data available 5. <code>--train_task</code>: Determine the task type, should be classification, multi-classification or regression 6. <code>--labels</code>: Set the labels for classification task, separated by <code>;</code> 7. <code>--run_name</code>: Name of the fine-tuned model 8. <code>--per_device_train_batch_size</code>: Batch size for training model 9. <code>--per_device_eval_batch_size</code>: Batch size for evaluating model 10. <code>--learning_rate</code>: Learning rate for training model 11. <code>--num_train_epochs</code>: Epoch for training model (also you can train model with steps, then you should change the strategies for save, logging and evaluation) 12. <code>--load_best_model_at_end</code>: Whether to load the model with the best performance on the evaluated data, default is <code>True</code> 13. <code>--metric_for_best_model</code>: Use which metric to determine the best model, default is <code>loss</code>, can be <code>accuracy</code>, <code>precison</code>, <code>recall</code>, <code>f1</code> or <code>matthews_correlation</code> for classification task, and <code>r2</code> or <code>spearmanr</code> for regression task 14. <code>--save_strategy</code>: Strategy for saving model, can be <code>epoch</code> or <code>steps</code> 15. <code>--logging_strategy</code>: Strategy for logging training information, can be <code>epoch</code> or <code>steps</code> 16. <code>--evaluation_strategy</code>: Strategy for evaluating model, can be <code>epoch</code> or <code>steps</code> 17. <code>--output_dir</code>: Where to save the fine-tuned model  </p> <p>Detailed descriptions of the arguments can be referred here.</p> <p>Finally, wait for the progress bar completed, and the fine-tuned model will be saved in the <code>plant-dnagpt-BPE-promoter</code> directory. In this directory, there will be a checkpoint directory, a runs directory, and a saved fine-tuning model.</p>"},{"location":"inference/","title":"Inference","text":"<p>To use a fine-tuned model for inference, please first download the desired models from HuggingFace or ModelScope to local or provide a model trained by yourself.</p> <ul> <li>Here is the finetune models list</li> </ul> <p>We use Plant DNAGPT model as example to predict active core promoter in plants.</p> <p>First download a fine-tuned model and corresponding dataset from HuggingFace or ModelScope</p> <pre><code># prepare a work directory\nmkdir inference\n# download fine-tuned model\ngit clone https://huggingface.co/zhangtaolab/plant-dnagpt-BPE-promoter models/plant-dnagpt-BPE-promoter\n# download train dataset\ngit clone https://huggingface.co/datasets/zhangtaolab/plant-multi-species-core-promoters data/plant-multi-species-core-promoters\n</code></pre> <p>We provide a script named <code>model_inference.py</code> for model inference. Here is an example that use the script to predict core promoter:</p> <pre><code># (method 1) Inference with local model, directly input a sequence\npython model_inference.py -m models/plant-dnagpt-BPE-promoter -s 'TTACTAAATTTATAACGATTTTTTATCTAACTTTAGCTCATCAATCTTTACCGTGTCAAAATTTAGTGCCAAGAAGCAGACATGGCCCGATGATCTTTTACCCTGTTTTCATAGCTCGCGAGCCGCGACCTGTGTCCAACCTCAACGGTCACTGCAGTCCCAGCACCTCAGCAGCCTGCGCCTGCCATACCCCCTCCCCCACCCACCCACACACACCATCCGGGCCCACGGTGGGACCCAGATGTCATGCGCTGTACGGGCGAGCAACTAGCCCCCACCTCTTCCCAAGAGGCAAAACCT'\n\n# (method 2) Inference with local model, provide a file contains multiple sequences to predict\npython model_inference.py -m models/plant-dnagpt-BPE-promoter -f data/plant-multi-species-core-promoters/test.csv -o inference/promoter_predict_results.txt\n\n# (method 3) Inference with an online model (Auto download the model trained by us from huggingface or modelscope)\npython model_inference.py -m zhangtaolab/plant-dnagpt-BPE-promoter -ms huggingface -s 'GGGAAAAAGTGAACTCCATTGTTTTTTCACGCTAAGCAGACCACAATTGCTGCTTGGTACGAAAAGAAAACCGAACCCTTTCACCCACGCACAACTCCATCTCCATTAGCATGGACAGAACACCGTAGATTGAACGCGGGAGGCAACAGGCTAAATCGTCCGTTCAGCCAAAACGGAATCATGGGCTGTTTTTCCAGAAGGCTCCGTGTCGTGTGGTTGTGGTCCAAAAACGAAAAAGAAAGAAAAAAGAAAACCCTTCCCAAGACGTGAAGAAAAGCAATGCGATGCTGATGCACGTTA'\n</code></pre> <p>In this script: 1. <code>-m</code>: Path to the fine-tuned model that is used for inference 2. <code>-s</code>: Input DNA sequence, only nucleotide A, C, G, T, N are acceptable 3. <code>-f</code>: Input file that contain multiple sequences, one line for each sequence. If you want to keep more information, file with <code>,</code> of <code>\\t</code> separator is acceptable, but a header contains <code>sequence</code> column must be specified. 4. <code>-ms</code>: Download the model from <code>huggingface</code> or <code>modelscope</code> if the model is not local. The format of model name is <code>zhangtaolab/model-name</code>, users can copy model name here: </p> <p>Output results contains the original sequence, input sequence length. If the task type is classification, predicted label and probability of each label will provide; If the task type is regression, a predicted score will provide.</p>"},{"location":"inference_api_for_developers/","title":"Inference API for developers","text":"<p>For developers who want to use our inference code in the Jupyter Notebook or other places, we developed a simple API package in the <code>pdllib</code>, which allows users directly call the inference function.</p> <p>Besides, we provide a Demo that shows the usage of our API, see notebook/inference_demo.ipynb.</p>"},{"location":"resources/finetune_models/","title":"Fine tune models","text":""},{"location":"resources/finetune_models/#finetune-models-list","title":"Finetune models list","text":"Model Tokenizer Task HuggingFace ModelScope Plant DNABERT BPE core promoter HuggingFace ModelScope Plant DNABERT BPE sequence conservation HuggingFace ModelScope Plant DNABERT BPE H3K27ac HuggingFace ModelScope Plant DNABERT BPE H3K27me3 HuggingFace ModelScope Plant DNABERT BPE H3K4me3 HuggingFace ModelScope Plant DNABERT BPE lncRNAs HuggingFace ModelScope Plant DNABERT BPE open chromatin HuggingFace ModelScope Plant DNABERT BPE promoter strength leaf HuggingFace ModelScope Plant DNABERT BPE promoter strength protoplast HuggingFace ModelScope Plant DNABERT 6mer core promoter HuggingFace ModelScope Plant DNABERT 6mer sequence conservation HuggingFace ModelScope Plant DNABERT 6mer H3K27ac HuggingFace ModelScope Plant DNABERT 6mer H3K27me3 HuggingFace ModelScope Plant DNABERT 6mer H3K4me3 HuggingFace ModelScope Plant DNABERT 6mer lncRNAs HuggingFace ModelScope Plant DNABERT 6mer open chromatin HuggingFace ModelScope Plant DNABERT 6mer promoter strength leaf HuggingFace ModelScope Plant DNABERT 6mer promoter strength protoplast HuggingFace ModelScope Plant DNABERT Single base core promoter HuggingFace ModelScope Plant DNABERT Single base sequence conservation HuggingFace ModelScope Plant DNABERT Single base H3K27ac HuggingFace ModelScope Plant DNABERT Single base H3K27me3 HuggingFace ModelScope Plant DNABERT Single base H3K4me3 HuggingFace ModelScope Plant DNABERT Single base lncRNAs HuggingFace ModelScope Plant DNABERT Single base open chromatin HuggingFace ModelScope Plant DNABERT Single base promoter strength leaf HuggingFace ModelScope Plant DNABERT Single base promoter strength protoplast HuggingFace ModelScope Plant Nucleotide Transformer BPE core promoter HuggingFace ModelScope Plant Nucleotide Transformer BPE sequence conservation HuggingFace ModelScope Plant Nucleotide Transformer BPE H3K27ac HuggingFace ModelScope Plant Nucleotide Transformer BPE H3K27me3 HuggingFace ModelScope Plant Nucleotide Transformer BPE H3K4me3 HuggingFace ModelScope Plant Nucleotide Transformer BPE lncRNAs HuggingFace ModelScope Plant Nucleotide Transformer BPE open chromatin HuggingFace ModelScope Plant Nucleotide Transformer BPE promoter strength leaf HuggingFace ModelScope Plant Nucleotide Transformer BPE promoter strength protoplast HuggingFace ModelScope Plant Nucleotide Transformer 6mer core promoter HuggingFace ModelScope Plant Nucleotide Transformer 6mer sequence conservation HuggingFace ModelScope Plant Nucleotide Transformer 6mer H3K27ac HuggingFace ModelScope Plant Nucleotide Transformer 6mer H3K27me3 HuggingFace ModelScope Plant Nucleotide Transformer 6mer H3K4me3 HuggingFace ModelScope Plant Nucleotide Transformer 6mer lncRNAs HuggingFace ModelScope Plant Nucleotide Transformer 6mer open chromatin HuggingFace ModelScope Plant Nucleotide Transformer 6mer promoter strength leaf HuggingFace ModelScope Plant Nucleotide Transformer 6mer promoter strength protoplast HuggingFace ModelScope Plant Nucleotide Transformer Single base core promoter HuggingFace ModelScope Plant Nucleotide Transformer Single base sequence conservation HuggingFace ModelScope Plant Nucleotide Transformer Single base H3K27ac HuggingFace ModelScope Plant Nucleotide Transformer Single base H3K27me3 HuggingFace ModelScope Plant Nucleotide Transformer Single base H3K4me3 HuggingFace ModelScope Plant Nucleotide Transformer Single base lncRNAs HuggingFace ModelScope Plant Nucleotide Transformer Single base open chromatin HuggingFace ModelScope Plant Nucleotide Transformer Single base promoter strength leaf HuggingFace ModelScope Plant Nucleotide Transformer Single base promoter strength protoplast HuggingFace ModelScope Plant DNAGPT BPE core promoter HuggingFace ModelScope Plant DNAGPT BPE sequence conservation HuggingFace ModelScope Plant DNAGPT BPE H3K27ac HuggingFace ModelScope Plant DNAGPT BPE H3K27me3 HuggingFace ModelScope Plant DNAGPT BPE H3K4me3 HuggingFace ModelScope Plant DNAGPT BPE lncRNAs HuggingFace ModelScope Plant DNAGPT BPE open chromatin HuggingFace ModelScope Plant DNAGPT BPE promoter strength leaf HuggingFace ModelScope Plant DNAGPT BPE promoter strength protoplast HuggingFace ModelScope Plant DNAGPT 6mer core promoter HuggingFace ModelScope Plant DNAGPT 6mer sequence conservation HuggingFace ModelScope Plant DNAGPT 6mer H3K27ac HuggingFace ModelScope Plant DNAGPT 6mer H3K27me3 HuggingFace ModelScope Plant DNAGPT 6mer H3K4me3 HuggingFace ModelScope Plant DNAGPT 6mer lncRNAs HuggingFace ModelScope Plant DNAGPT 6mer open chromatin HuggingFace ModelScope Plant DNAGPT 6mer promoter strength leaf HuggingFace ModelScope Plant DNAGPT 6mer promoter strength protoplast HuggingFace ModelScope Plant DNAGPT Single base core promoter HuggingFace ModelScope Plant DNAGPT Single base sequence conservation HuggingFace ModelScope Plant DNAGPT Single base H3K27ac HuggingFace ModelScope Plant DNAGPT Single base H3K27me3 HuggingFace ModelScope Plant DNAGPT Single base H3K4me3 HuggingFace ModelScope Plant DNAGPT Single base lncRNAs HuggingFace ModelScope Plant DNAGPT Single base open chromatin HuggingFace ModelScope Plant DNAGPT Single base promoter strength leaf HuggingFace ModelScope Plant DNAGPT Single base promoter strength protoplast HuggingFace ModelScope Plant DNAGemma BPE core promoter HuggingFace ModelScope Plant DNAGemma BPE sequence conservation HuggingFace ModelScope Plant DNAGemma BPE H3K27ac HuggingFace ModelScope Plant DNAGemma BPE H3K27me3 HuggingFace ModelScope Plant DNAGemma BPE H3K4me3 HuggingFace ModelScope Plant DNAGemma BPE lncRNAs HuggingFace ModelScope Plant DNAGemma BPE open chromatin HuggingFace ModelScope Plant DNAGemma BPE promoter strength leaf HuggingFace ModelScope Plant DNAGemma BPE promoter strength protoplast HuggingFace ModelScope Plant DNAGemma 6mer core promoter HuggingFace ModelScope Plant DNAGemma 6mer sequence conservation HuggingFace ModelScope Plant DNAGemma 6mer H3K27ac HuggingFace ModelScope Plant DNAGemma 6mer H3K27me3 HuggingFace ModelScope Plant DNAGemma 6mer H3K4me3 HuggingFace ModelScope Plant DNAGemma 6mer lncRNAs HuggingFace ModelScope Plant DNAGemma 6mer open chromatin HuggingFace ModelScope Plant DNAGemma 6mer promoter strength leaf HuggingFace ModelScope Plant DNAGemma 6mer promoter strength protoplast HuggingFace ModelScope Plant DNAGemma Single base core promoter HuggingFace ModelScope Plant DNAGemma Single base sequence conservation HuggingFace ModelScope Plant DNAGemma Single base H3K27ac HuggingFace ModelScope Plant DNAGemma Single base H3K27me3 HuggingFace ModelScope Plant DNAGemma Single base H3K4me3 HuggingFace ModelScope Plant DNAGemma Single base lncRNAs HuggingFace ModelScope Plant DNAGemma Single base open chromatin HuggingFace ModelScope Plant DNAGemma Single base promoter strength leaf HuggingFace ModelScope Plant DNAGemma Single base promoter strength protoplast HuggingFace ModelScope Plant DNAMamba BPE core promoter HuggingFace ModelScope Plant DNAMamba BPE sequence conservation HuggingFace ModelScope Plant DNAMamba BPE H3K27ac HuggingFace ModelScope Plant DNAMamba BPE H3K27me3 HuggingFace ModelScope Plant DNAMamba BPE H3K4me3 HuggingFace ModelScope Plant DNAMamba BPE lncRNAs HuggingFace ModelScope Plant DNAMamba BPE open chromatin HuggingFace ModelScope Plant DNAMamba BPE promoter strength leaf HuggingFace ModelScope Plant DNAMamba BPE promoter strength protoplast HuggingFace ModelScope Plant DNAMamba 2mer core promoter HuggingFace ModelScope Plant DNAMamba 2mer sequence conservation HuggingFace ModelScope Plant DNAMamba 2mer H3K27ac HuggingFace ModelScope Plant DNAMamba 2mer H3K27me3 HuggingFace ModelScope Plant DNAMamba 2mer H3K4me3 HuggingFace ModelScope Plant DNAMamba 2mer lncRNAs HuggingFace ModelScope Plant DNAMamba 2mer open chromatin HuggingFace ModelScope Plant DNAMamba 2mer promoter strength leaf HuggingFace ModelScope Plant DNAMamba 2mer promoter strength protoplast HuggingFace ModelScope Plant DNAMamba 3mer core promoter HuggingFace ModelScope Plant DNAMamba 3mer sequence conservation HuggingFace ModelScope Plant DNAMamba 3mer H3K27ac HuggingFace ModelScope Plant DNAMamba 3mer H3K27me3 HuggingFace ModelScope Plant DNAMamba 3mer H3K4me3 HuggingFace ModelScope Plant DNAMamba 3mer lncRNAs HuggingFace ModelScope Plant DNAMamba 3mer open chromatin HuggingFace ModelScope Plant DNAMamba 3mer promoter strength leaf HuggingFace ModelScope Plant DNAMamba 3mer promoter strength protoplast HuggingFace ModelScope Plant DNAMamba 4mer core promoter HuggingFace ModelScope Plant DNAMamba 4mer sequence conservation HuggingFace ModelScope Plant DNAMamba 4mer H3K27ac HuggingFace ModelScope Plant DNAMamba 4mer H3K27me3 HuggingFace ModelScope Plant DNAMamba 4mer H3K4me3 HuggingFace ModelScope Plant DNAMamba 4mer lncRNAs HuggingFace ModelScope Plant DNAMamba 4mer open chromatin HuggingFace ModelScope Plant DNAMamba 4mer promoter strength leaf HuggingFace ModelScope Plant DNAMamba 4mer promoter strength protoplast HuggingFace ModelScope Plant DNAMamba 5mer core promoter HuggingFace ModelScope Plant DNAMamba 5mer sequence conservation HuggingFace ModelScope Plant DNAMamba 5mer H3K27ac HuggingFace ModelScope Plant DNAMamba 5mer H3K27me3 HuggingFace ModelScope Plant DNAMamba 5mer H3K4me3 HuggingFace ModelScope Plant DNAMamba 5mer lncRNAs HuggingFace ModelScope Plant DNAMamba 5mer open chromatin HuggingFace ModelScope Plant DNAMamba 5mer promoter strength leaf HuggingFace ModelScope Plant DNAMamba 5mer promoter strength protoplast HuggingFace ModelScope Plant DNAMamba 6mer core promoter HuggingFace ModelScope Plant DNAMamba 6mer sequence conservation HuggingFace ModelScope Plant DNAMamba 6mer H3K27ac HuggingFace ModelScope Plant DNAMamba 6mer H3K27me3 HuggingFace ModelScope Plant DNAMamba 6mer H3K4me3 HuggingFace ModelScope Plant DNAMamba 6mer lncRNAs HuggingFace ModelScope Plant DNAMamba 6mer open chromatin HuggingFace ModelScope Plant DNAMamba 6mer promoter strength leaf HuggingFace ModelScope Plant DNAMamba 6mer promoter strength protoplast HuggingFace ModelScope Plant DNAMamba Single base core promoter HuggingFace ModelScope Plant DNAMamba Single base sequence conservation HuggingFace ModelScope Plant DNAMamba Single base H3K27ac HuggingFace ModelScope Plant DNAMamba Single base H3K27me3 HuggingFace ModelScope Plant DNAMamba Single base H3K4me3 HuggingFace ModelScope Plant DNAMamba Single base lncRNAs HuggingFace ModelScope Plant DNAMamba Single base open chromatin HuggingFace ModelScope Plant DNAMamba Single base promoter strength leaf HuggingFace ModelScope Plant DNAMamba Single base promoter strength protoplast HuggingFace ModelScope DNABERT-2 BPE core promoter HuggingFace ModelScope DNABERT-2 BPE sequence conservation HuggingFace ModelScope DNABERT-2 BPE H3K27ac HuggingFace ModelScope DNABERT-2 BPE H3K27me3 HuggingFace ModelScope DNABERT-2 BPE H3K4me3 HuggingFace ModelScope DNABERT-2 BPE lncRNAs HuggingFace ModelScope DNABERT-2 BPE open chromatin HuggingFace ModelScope DNABERT-2 BPE promoter strength leaf HuggingFace ModelScope DNABERT-2 BPE promoter strength protoplast HuggingFace ModelScope Nucleotide Transformer v2 100m 6mer core promoter HuggingFace ModelScope Nucleotide Transformer v2 100m 6mer sequence conservation HuggingFace ModelScope Nucleotide Transformer v2 100m 6mer H3K27ac HuggingFace ModelScope Nucleotide Transformer v2 100m 6mer H3K27me3 HuggingFace ModelScope Nucleotide Transformer v2 100m 6mer H3K4me3 HuggingFace ModelScope Nucleotide Transformer v2 100m 6mer lncRNAs HuggingFace ModelScope Nucleotide Transformer v2 100m 6mer open chromatin HuggingFace ModelScope Nucleotide Transformer v2 100m 6mer promoter strength leaf HuggingFace ModelScope Nucleotide Transformer v2 100m 6mer promoter strength protoplast HuggingFace ModelScope AgroNT-1b 6mer core promoter HuggingFace ModelScope AgroNT-1b 6mer sequence conservation HuggingFace ModelScope AgroNT-1b 6mer H3K27ac HuggingFace ModelScope AgroNT-1b 6mer H3K27me3 HuggingFace ModelScope AgroNT-1b 6mer H3K4me3 HuggingFace ModelScope AgroNT-1b 6mer lncRNAs HuggingFace ModelScope AgroNT-1b 6mer open chromatin HuggingFace ModelScope AgroNT-1b 6mer promoter strength leaf HuggingFace ModelScope AgroNT-1b 6mer promoter strength protoplast HuggingFace ModelScope"},{"location":"resources/platforms/","title":"Platforms","text":""},{"location":"resources/platforms/#online-prediction-platform","title":"Online Prediction Platform","text":"<p>In order to facilitate users to use the model to predict DNA analysis tasks, we also provide online prediction platforms.</p> <p>Here are the prediction websites for different tasks:</p> Model Task Tokenizer HuggingFace ModelScope multiple core promoter BPE HuggingFace ModelScope multiple sequence conservation BPE HuggingFace ModelScope multiple H3K27ac BPE HuggingFace ModelScope multiple H3K27me3 BPE HuggingFace ModelScope multiple H3K4me3 BPE HuggingFace ModelScope multiple lncRNAs BPE HuggingFace ModelScope multiple open chromatin BPE HuggingFace ModelScope multiple promoter strength leaf BPE HuggingFace ModelScope multiple promoter strength protoplast BPE HuggingFace ModelScope multiple core promoter 6mer HuggingFace ModelScope multiple sequence conservation 6mer HuggingFace ModelScope multiple H3K27ac 6mer HuggingFace ModelScope multiple H3K27me3 6mer HuggingFace ModelScope multiple H3K4me3 6mer HuggingFace ModelScope multiple lncRNAs 6mer HuggingFace ModelScope multiple open chromatin 6mer HuggingFace ModelScope multiple promoter strength leaf 6mer HuggingFace ModelScope multiple promoter strength protoplast 6mer HuggingFace ModelScope multiple core promoter Single base HuggingFace ModelScope multiple sequence conservation Single base HuggingFace ModelScope multiple H3K27ac Single base HuggingFace ModelScope multiple H3K27me3 Single base HuggingFace ModelScope multiple H3K4me3 Single base HuggingFace ModelScope multiple lncRNAs Single base HuggingFace ModelScope multiple open chromatin Single base HuggingFace ModelScope multiple promoter strength leaf Single base HuggingFace ModelScope multiple promoter strength protoplast Single base HuggingFace ModelScope Plant DNAMamba multiple multiple https://bioinfor.yzu.edu.cn/llms/dnamamba https://llms.zhangtaolab.org/llms/dnamamba Plant DNAMamba open chromatin BPE https://bioinfor.yzu.edu.cn/llms/open-chromatin https://llms.zhangtaolab.org/llms/open-chromatin <p>Here is a preview of prediction of open chromatin by plant DNAMamba model Preview\uff1a</p> <p></p>"},{"location":"resources/pretrain_models/","title":"Pretrain models","text":""},{"location":"resources/pretrain_models/#pretrain-models-list","title":"Pretrain models list","text":"Model Tokenizer HuggingFace ModelScope Plant DNABERT BPE HuggingFace ModelScope Plant DNABERT 6mer HuggingFace ModelScope Plant DNABERT Single base HuggingFace ModelScope Plant Nucleotide Transformer BPE HuggingFace ModelScope Plant Nucleotide Transformer 6mer HuggingFace ModelScope Plant Nucleotide Transformer Single base HuggingFace ModelScope Plant DNAGPT BPE HuggingFace ModelScope Plant DNAGPT 6mer HuggingFace ModelScope Plant DNAGPT Single base HuggingFace ModelScope Plant DNAGemma BPE HuggingFace ModelScope Plant DNAGemma 6mer HuggingFace ModelScope Plant DNAGemma Single base HuggingFace ModelScope Plant DNAMamba BPE HuggingFace ModelScope Plant DNAMamba 2mer HuggingFace ModelScope Plant DNAMamba 3mer HuggingFace ModelScope Plant DNAMamba 4mer HuggingFace ModelScope Plant DNAMamba 5mer HuggingFace ModelScope Plant DNAMamba 6mer HuggingFace ModelScope Plant DNAMamba Single base HuggingFace ModelScope"}]}